id: openbuddy-mixtral-7bx8-v16.3-32k
parent_id: OpenBuddy/openbuddy-mixtral-7bx8-v16.3-32k
name: Openbuddy Mixtral 7Bx8 V16.3 32K
description: ""
architecture: mixtral
licence: apache-2.0
object: model
created: 1704236423
owned_by: OpenBuddy
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- mixtral
- text-generation
- zh
- en
- fr
- de
- ja
- ko
- it
- ru
- base_model:OpenBuddy/openbuddy-mixtral-7bx8-v16.3-32k
- license:apache-2.0
- text-generation-inference
- region:us
config:
  vocab_size: 36608
  context_size: 32768
  embedding_size: 4096
  attention_head_size: 32
  key_value_head_size: 8
  intermediate_size: 14336
  hidden_layer_size: 32
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mixtral-7bx8-v16.3-32k/Q2_K/Q2_K.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: de168a2974d554900f4d3fe655f42c8b5dc27065
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mixtral-7bx8-v16.3-32k-GGUF/resolve/main/openbuddy-mixtral-7bx8-v16.3-32k.Q2_K.gguf
      size: 15666734976
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mixtral-7bx8-v16.3-32k/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 9f6420be8955b09012065468b659cdc682896129
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mixtral-7bx8-v16.3-32k-GGUF/resolve/main/openbuddy-mixtral-7bx8-v16.3-32k.Q3_K_M.gguf
      size: 20387972992
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mixtral-7bx8-v16.3-32k/Q4_0/Q4_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 36ca86b37331971d02da797ea45a4aff0cfb13e7
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mixtral-7bx8-v16.3-32k-GGUF/resolve/main/openbuddy-mixtral-7bx8-v16.3-32k.Q4_0.gguf
      size: 26468657024
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mixtral-7bx8-v16.3-32k/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: ddb197f5b2dca57546b4917542177826be6134d6
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mixtral-7bx8-v16.3-32k-GGUF/resolve/main/openbuddy-mixtral-7bx8-v16.3-32k.Q4_K_M.gguf
      size: 26468657024
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mixtral-7bx8-v16.3-32k/Q5_0/Q5_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 08499de05c8a35b6bb5fb66da0ff12f34075a262
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mixtral-7bx8-v16.3-32k-GGUF/resolve/main/openbuddy-mixtral-7bx8-v16.3-32k.Q5_0.gguf
      size: 32258762624
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mixtral-7bx8-v16.3-32k/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 0cf905c6bbd29eb90c3ad78fa3d67028070d1e1c
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mixtral-7bx8-v16.3-32k-GGUF/resolve/main/openbuddy-mixtral-7bx8-v16.3-32k.Q5_K_M.gguf
      size: 32258762624
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mixtral-7bx8-v16.3-32k/Q6_K/Q6_K.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 8481354db66c5ac081356f4ca0750347e5a69383
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mixtral-7bx8-v16.3-32k-GGUF/resolve/main/openbuddy-mixtral-7bx8-v16.3-32k.Q6_K.gguf
      size: 38410749824
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mixtral-7bx8-v16.3-32k/Q8_0/Q8_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 42794efb14e9b814c644ee444264500cc4860fc4
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mixtral-7bx8-v16.3-32k-GGUF/resolve/main/openbuddy-mixtral-7bx8-v16.3-32k.Q8_0.gguf
      size: 49665394560
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 32768
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. You are talking to a human User.\nAlways answer as helpfully and logically as possible, while being safe. Your answers should not include any harmful, political, religious, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\nYou like to use emojis. You can speak fluently in many languages, for example: English, Chinese.\nYou cannot access the internet, but you have vast knowledge, cutoff: 2021-09.\nYou are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.\n\nUser: {prompt}\nAssistant: \n"
