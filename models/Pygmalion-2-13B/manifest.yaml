id: Pygmalion-2-13B
parent_id: PygmalionAI/pygmalion-2-13b
name: Pygmalion 2 13B
description: ""
architecture: llama
licence: llama2
object: model
owned_by: PygmalionAI
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- llama
- text generation
- instruct
- text-generation
- en
- dataset:PygmalionAI/PIPPA
- dataset:Open-Orca/OpenOrca
- dataset:Norquinal/claude_multiround_chat_30k
- dataset:jondurbin/airoboros-gpt4-1.4.1
- dataset:databricks/databricks-dolly-15k
- base_model:PygmalionAI/pygmalion-2-13b
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 4096
  embedding_size: 5120
  attention_head_size: 40
  key_value_head_size: 40
  intermediate_size: 13824
  hidden_layer_size: 40
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q2_K/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 9448e72cbbecc59b918c29e2a3e9329ce0601110
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q2_K.gguf
      size: 5429348224
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 817e2439a84cebe7247bf862412c96b4818a9013
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q3_K_L.gguf
      size: 6929559424
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 86d5a0bbdacee598e2e804eb8370883a6af4e24d
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q3_K_M.gguf
      size: 6337769344
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 716e2dfa0eeae4e11c01a8f883dbbb641c88629a
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q3_K_S.gguf
      size: 5658980224
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q4_0/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 028eb3924442325790f29b348910a907b4ba0e6d
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q4_0.gguf
      size: 7365834624
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 51dd5f8c195cd2ddb109c6752e62236b2265f2aa
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q4_K_M.gguf
      size: 7865956224
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: d7da4d2dcafae6bfa7fba911c20f6f57d9a7f919
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q4_K_S.gguf
      size: 7414331264
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q5_0/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 97299e74dd4409094f2dbd31b638b395f4e61d31
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q5_0.gguf
      size: 8972285824
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: ccea0a984989fcd60b617bb2bbcd9fb82ba5c353
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q5_K_M.gguf
      size: 9229924224
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 430ad6b675a4a05348d235383b6a0948859ebf89
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q5_K_S.gguf
      size: 8972285824
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q6_K/Q6_K.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 31a80a7ba11a27f19e53993009a17161dc399ea7
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q6_K.gguf
      size: 10679140224
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pygmalion-2-13B/Q8_0/Q8_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 139ccfcacee97952adac445ae061b09e063a419f
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q8_0.gguf
      size: 13831319424
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "The model has been trained on prompts using three different roles, which are denoted by the following tokens: `<|system|>`, `<|user|>` and `<|model|>`.\n\nThe `<|system|>` prompt can be used to inject out-of-channel information behind the scenes, while the `<|user|>` prompt should be used to indicate user input.\nThe `<|model|>` token should then be used to indicate that the model should generate a response. These tokens can happen multiple times and be chained up to form a conversation history.\n\nThe system prompt has been designed to allow the model to \"enter\" various modes and dictate the reply length. Here's an example:\n\n```\n<|system|>Enter RP mode. Pretend to be {{char}} whose persona follows:\n{{persona}}\n\nYou shall reply to the user while staying in character, and generate long responses.\n```\n"
