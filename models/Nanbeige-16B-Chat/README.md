---
license: other
language:
- en
- zh
library_name: transformers
pipeline_tag: text-generation
tags:
- llm
- Nanbeige
- custom_code
extra_gated_prompt: "è®¿é—®æ­¤æ¨¡å‹éœ€è¦é˜…è¯»å¹¶åŒæ„ä»¥ä¸‹åè®®[è¿™é‡Œ](https://github.com/Nanbeige/Nanbeige/blob/main/%E5%8D%97%E5%8C%97%E9%98%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)\nAccess to this model requires reading and agreeing to the following agreement [here](https://github.com/Nanbeige/Nanbeige/blob/main/License_Agreement_for_Large_Language_Models_Nanbeige.pdf)"
extra_gated_fields:
 Name: text
 Country: text
 Affiliation: text
 Email: text
 I agree to the license terms described in the above agreement: checkbox
---

<!-- markdownlint-disable first-line-h1 -->

<!-- markdownlint-disable html -->

<div align="center">
<h1>
  Nanbeige-16B-Chat
</h1>
</div>

<p align="center">
  <a href="https://github.com/Nanbeige/Nanbeige" target="_blank">ğŸ’»Github</a>
</p>

# <span id="Introduction">æ¨¡å‹ä»‹ç»ï¼ˆIntroductionï¼‰</span>

Nanbeige-16Bï¼ˆå—åŒ—é˜-16Bï¼‰æ˜¯å—åŒ—é˜å¤§æ¨¡å‹å®éªŒå®¤ç ”å‘çš„160äº¿å‚æ•°è§„æ¨¡çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨äº†2.5T Tokensè¿›è¡Œé¢„è®­ç»ƒï¼Œæ•°æ®åŒ…å«å¤§é‡äº’è”ç½‘é«˜è´¨é‡è¯­æ–™ã€å„ç±»ä¹¦ç±ã€ä»£ç ç­‰é¢†åŸŸè„±æ•æ–‡æœ¬ï¼Œåœ¨å„ä¸ªæƒå¨æµ‹è¯„æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰ Baseã€Chat ä»¥åŠæ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦çš„ Base-32kã€Chat-32k ç‰ˆæœ¬ã€‚

Base-32k ç‰ˆæœ¬åŸºäºNanbeige-16B-Baseæ¨¡å‹ï¼Œé‡‡ç”¨YaRNæ’å€¼æ–¹æ³•å¯¹ä½ç½®ç¼–ç è¿›è¡Œä¿®æ”¹ï¼Œå¹¶ä»¥32kä¸ºæœ€å¤§é•¿åº¦è¿›è¡Œäº†20B Tokensçš„ä¸­æ–‡ã€è‹±æ–‡ã€ä»£ç è¯­æ–™çš„å…¨å‚æ•°å¢é‡é¢„è®­ç»ƒã€‚

Chat ç‰ˆæœ¬å’Œ Chat-32k ç‰ˆæœ¬åˆ†åˆ«åŸºäºNanbeige-16B-Baseæ¨¡å‹å’ŒNanbeige-16B-Base-32kæ¨¡å‹ï¼Œç»è¿‡äº†å¤§é‡äººç±»å¯¹é½è®­ç»ƒï¼Œèƒ½å¤Ÿæ›´å¥½ã€æ›´å®‰å…¨åœ°å›å¤ç”¨æˆ·çš„é—®é¢˜ã€‚

å¦‚æœæ‚¨éœ€è¦å¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬æ¨èæ‚¨ä½¿ç”¨Nanbeige-16B-Base-32kå’ŒNanbeige-16B-Chat-32kã€‚

æœ¬ä»“åº“ä¸º Nanbeige-16B-Chat çš„æ¨¡å‹ä»“åº“ã€‚

Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.

The Base-32k version is based on the Nanbeige-16B-Base model, which uses YaRN interpolation method to modify the position encoding, and performs full parameter incremental pre-training with 20 billion tokens of Chinese, English, and code corpora, under 32k max length.

The Chat version and Chat-32k version are based on the Nanbeige-16B-Base model and Nanbeige-16B-Base-32k model respectively. They have undergone extensive human-aligned training.

If you need to deal with longer contexts, we recommend using Nanbeige-16B-Base-32k and Nanbeige-16B-Chat-32k.

This repository is the one for Nanbeige-16B-Chat model.

##
|         | Base Model  | Base-32k Model | Chat Model | Chat-32k Model |
|:-------:|:-------:|:-------:|:---------:|:--------:|
| 16B     | ğŸ¤— [Nanbeige-16B-Base](https://huggingface.co/Nanbeige/Nanbeige-16B-Base) | ğŸ¤— [Nanbeige-16B-Base-32k](https://huggingface.co/Nanbeige/Nanbeige-16B-Base-32k) | ğŸ¤— [Nanbeige-16B-Chat](https://huggingface.co/Nanbeige/Nanbeige-16B-Chat) |ğŸ¤— [Nanbeige-16B-Chat-32k](https://huggingface.co/Nanbeige/Nanbeige-16B-Chat-32k) |


##
# <span id="Inference">æ¨¡å‹æ¨ç† (Inference)</span>

## ç›¸å…³ä¾èµ–

- python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
  
- transformers 4.33.3
  
- pytorch 2.0åŠä»¥ä¸Šç‰ˆæœ¬
  
- python 3.8 and above
  
- transformers 4.33.3
  
- pytorch 2.0åŠä»¥ä¸Šç‰ˆæœ¬

å¯ä»¥é€šè¿‡ä»¥ä¸‹pipå‘½ä»¤å®‰è£…ç›¸å…³ä¾èµ–åº“

You can install the dependent libraries with the following pip command

```
pip install transformers==4.33.3 transformers_stream_generator deepspeed einops==0.3.2 datasets==2.10.0
```

## æ¨ç†ä»£ç 

é€šè¿‡ä»¥ä¸‹ä»£ç å¯ä»¥è°ƒç”¨æ¨¡å‹è¿›è¡ŒèŠå¤©å¯¹è¯ï¼š

The following code can be used to invoke the model for chat dialogue:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Nanbeige/Nanbeige-16B-Chat", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Nanbeige/Nanbeige-16B-Chat", device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True)

question = "ä½ å¯ä»¥ç»™æˆ‘ä¸€äº›å…·ä½“çš„SEOä¼˜åŒ–æŠ€å·§å—ï¼Ÿ"

output, messages = model.chat(tokenizer, question)
print(output)
```

##
# <span id="Evaluation">æ€§èƒ½æµ‹è¯„ï¼ˆPerformance Evaluationï¼‰</span>

### LLMEval-3
**LLMEval-3** ( [Github](https://github.com/llmeval/llmeval-3) / [ä¸»é¡µ](http://llmeval.com/index) ) èšç„¦äºä¸“ä¸šçŸ¥è¯†èƒ½åŠ›è¯„æµ‹ï¼Œæ¶µç›–å“²å­¦ã€ç»æµå­¦ã€æ³•å­¦ã€æ•™è‚²å­¦ã€æ–‡å­¦ã€å†å²å­¦ã€ç†å­¦ã€å·¥å­¦ã€å†œå­¦ã€åŒ»å­¦ã€å†›äº‹å­¦ã€ç®¡ç†å­¦ã€è‰ºæœ¯å­¦ç­‰æ•™è‚²éƒ¨åˆ’å®šçš„13ä¸ªå­¦ç§‘é—¨ç±»ã€50ä½™ä¸ªäºŒçº§å­¦ç§‘ï¼Œå…±è®¡çº¦20Wé“æ ‡å‡†ç”Ÿæˆå¼é—®ç­”é¢˜ç›®ã€‚é˜²æ­¢ä½œå¼Šæ˜¯LLMEval-3è€ƒè™‘çš„é‡è¦å› ç´ ã€‚ç°æœ‰å…¬å¼€è¯„æµ‹åŸºå‡†å­˜åœ¨æµ‹è¯•é¢˜åº“æ³„éœ²çš„é—®é¢˜ï¼Œå› æ­¤å¯èƒ½å‡ºç°â€œåˆ·æ¦œâ€ã€â€œåˆ·åˆ†â€ç­‰ä¸å…¬å¹³ç°è±¡ï¼Œåœ¨LLMEval-3ä¸­ï¼Œæ¯ä¸ªå‚ä¸è¯„æµ‹çš„ç³»ç»Ÿéœ€è¦å®Œæˆä»æ€»é¢˜åº“ä¸­éšæœºæŠ½æ ·çš„1000é¢˜ï¼Œé’ˆå¯¹åŒä¸€æœºæ„çš„æ¨¡å‹ï¼Œç¡®ä¿æ¯æ¬¡è¯„æµ‹é¢˜ç›®ä¸é‡å¤ã€‚

æˆ‘ä»¬åŸºäº LLMEval-3 å¯¹ Nanbeige-16B-Chat æ¨¡å‹è¿›è¡Œäº†å…¨é¢æµ‹è¯„ï¼Œæµ‹è¯„ç»“æœå¦‚ä¸‹ï¼š

We conducted a comprehensive evaluation of Nanbeige-16B-Chat model based on **LLMEval-3** ( [Github](https://github.com/llmeval/llmeval-3) / [Homepage](http://llmeval.com/index) ), and the results are as follows:

| æ¨¡å‹åç§°                       | ç›¸å¯¹åˆ†æ•°-GPT4 | ç›¸å¯¹åˆ†æ•°-GPT3.5 | ç»å¯¹åˆ†æ•°  | å·¥å­¦   | ç»æµå­¦  | æ•™è‚²å­¦  | æ³•å­¦   | æ–‡å­¦   | ç®¡ç†å­¦  | ç†å­¦   | å†å²å­¦  | åŒ»å­¦   | å†›äº‹å­¦  |
|----------------------------|-----------|-------------|-------|------|------|------|------|------|------|------|------|------|------|
| Baidu3.5                   | 104.21    | 121.39      | 77.53 | 8.13 | 8.00 | 8.63 | 7.97 | 6.23 | 7.63 | 7.33 | 8.77 | 7.47 | 7.37 |
| ChatGLM-pro                | 103.45    | 120.51      | 76.97 | 6.97 | 8.47 | 7.97 | 8.93 | 7.23 | 7.70 | 6.33 | 8.37 | 7.13 | 7.87 |
| GPT-4                      | 100.00    | 116.49      | 74.40 | 7.23 | 7.80 | 7.73 | 8.40 | 6.73 | 7.67 | 7.73 | 7.07 | 6.20 | 7.83 |
| **Nanbeige-16B-Chat**               | 94.26     | 109.80      | 70.13 | 6.00 | 7.87 | 8.20 | 8.33 | 6.07 | 6.83 | 6.00 | 7.80 | 5.80 | 7.23 |
| minimax-abab5              | 93.28     | 108.66      | 69.40 | 5.83 | 7.50 | 7.77 | 8.37 | 6.40 | 6.33 | 5.07 | 8.33 | 5.93 | 7.87 |
| Baichuan2-13B-Chat         | 92.91     | 108.23      | 69.13 | 6.00 | 7.53 | 8.63 | 8.13 | 6.23 | 6.33 | 5.63 | 8.20 | 5.43 | 7.00 |
| Qwen-14B-Chat              | 86.33     | 100.56      | 64.23 | 5.77 | 7.07 | 7.07 | 7.37 | 5.70 | 6.20 | 5.93 | 6.97 | 5.40 | 6.77 |
| GPT-3.5-turbo              | 85.84     | 100.00      | 63.87 | 6.27 | 6.87 | 7.23 | 7.40 | 5.40 | 6.30 | 6.37 | 6.00 | 5.17 | 6.87 |
| ChatGLM2-6B                | 75.71     | 88.19       | 56.33 | 4.03 | 6.33 | 7.00 | 7.57 | 4.77 | 5.93 | 4.23 | 5.87 | 5.07 | 5.53 |
| ziya_v1.1-13b              | 70.20     | 81.78       | 52.23 | 4.67 | 5.77 | 6.07 | 6.53 | 4.53 | 5.33 | 3.70 | 5.00 | 4.63 | 6.00 |
| BELLE-Llama2-13B-chat-0.4M | 68.82     | 80.16       | 51.20 | 4.47 | 5.93 | 6.20 | 6.77 | 4.33 | 4.97 | 4.10 | 5.07 | 3.77 | 5.60 |
| Linly-Chinese-LLaMA2-13B   | 67.82     | 79.00       | 50.46 | 3.87 | 5.80 | 5.83 | 6.57 | 3.93 | 5.37 | 4.07 | 5.43 | 3.93 | 5.67 |
| InternLM-Chat-7B           | 61.06     | 71.13       | 45.43 | 3.83 | 5.13 | 5.27 | 6.57 | 3.90 | 4.83 | 3.10 | 4.87 | 3.67 | 4.27 |
| Llama-2-7b-chat-hf         | 51.11     | 59.54       | 38.03 | 3.33 | 4.77 | 3.77 | 5.03 | 3.07 | 3.77 | 3.93 | 4.00 | 2.40 | 3.97 |


##
# <span id="Limitations">å±€é™æ€§ï¼ˆLimitationsï¼‰</span>

è™½ç„¶æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éå¸¸æ³¨é‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼ŒåŠ›æ±‚ç¡®ä¿å…¶è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†ç”±äºæ¨¡å‹å¤§å°å’Œæ¦‚ç‡ç”ŸæˆèŒƒå¼çš„é™åˆ¶ï¼Œæ— æ³•å®Œå…¨é¿å…äº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºæƒ…å†µã€‚è¿™äº›è¾“å‡ºå¯èƒ½åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚æˆ‘ä»¬ä¸æ‰¿æ‹…å› ä¼ æ’­ä¸è‰¯ä¿¡æ¯è€Œå¯¼è‡´çš„ä»»ä½•åæœã€‚

While we place great emphasis on the safety of the model during the training process, striving to ensure that its outputs align with ethical and legal requirements, it may not completely avoid generating unexpected outputs due to the model's size and probabilistic nature. These outputs may include harmful content such as bias or discrimination. Please don't propagate such content. We do not assume any responsibility for the consequences resulting from the dissemination of inappropriate information.

# <span id="License">åè®®ï¼ˆLicenseï¼‰</span>

ä½¿ç”¨ Nanbeige æ¨¡å‹æ—¶ï¼Œæ‚¨å¿…é¡»éµå®ˆ Apache 2.0 è®¸å¯è¯å’Œ[ã€Šå—åŒ—é˜å¤§è¯­è¨€æ¨¡å‹è®¸å¯åè®®ã€‹](https://huggingface.co/Nanbeige/Nanbeige-16B-Chat/resolve/main/å—åŒ—é˜å¤§è¯­è¨€æ¨¡å‹è®¸å¯åè®®.pdf)ã€‚å¦‚æœæ‚¨æ‰“ç®—å°† Nanbeige æ¨¡å‹æˆ–å…¶è¡ç”Ÿäº§å“ç”¨äºå•†ä¸šç›®çš„ï¼Œè¯·é€šè¿‡ä»¥ä¸‹è”ç³»é‚®ç®± nanbeige@126.com æäº¤ç”³è¯·ææ–™ï¼Œä»¥æ»¡è¶³ã€Šå—åŒ—é˜å¤§è¯­è¨€æ¨¡å‹è®¸å¯åè®®ã€‹çš„è¦æ±‚ã€‚ç»è¿‡å®¡æ ¸åï¼Œæˆ‘ä»¬å°†æˆäºˆæ‚¨éæ’ä»–æ€§ã€å…¨çƒèŒƒå›´å†…ã€ä¸å¯è½¬è®©ã€ä¸å¯å†è®¸å¯ã€å¯æ’¤é”€çš„å•†ä¸šç‰ˆæƒè®¸å¯ã€‚

When using the Nanbeige models, you must comply with the Apache 2.0 License and the [License Agreement for Large Language Models Nanbeige](https://huggingface.co/Nanbeige/Nanbeige-16B-Chat/resolve/main/License_Agreement_for_Large_Language_Models_Nanbeige.pdf). If you intend to use the Nanbeige Models or its derivatives for commercial purposes, please submit application materials to meet the requirements of the Nanbeige Models Community License Agreement by contacting nanbeige@126.com. After review, We will grant you a non-exclusive, worldwide, non-transferable, non-sublicensable and revocable commercial copyright license.
