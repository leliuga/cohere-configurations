id: Airoboros-L2-7B-2.1
parent_id: jondurbin/airoboros-l2-7b-2.1
name: Airoboros L2 7B 2.1
description: ""
architecture: llama
licence: llama2
object: model
owned_by: Jon Durbin
pipeline: ""
languages: []
tags:
- transformers
- gguf
- llama
- dataset:jondurbin/airoboros-2.1
- base_model:jondurbin/airoboros-l2-7b-2.1
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 4096
  embedding_size: 4096
  attention_head_size: 32
  key_value_head_size: 32
  intermediate_size: 11008
  hidden_layer_size: 32
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q2_K/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 5a5b2dfeb250af00b0af7a0fd792b02c0f4ac095
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q2_K.gguf
      size: 2825940672
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 8a23b12774b6180310647b6b86d34cc53e05faec
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q3_K_L.gguf
      size: 3597110976
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 43f1f57fc74bd5de33b58659383ac235fea5fb0c
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q3_K_M.gguf
      size: 3298004672
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 30956d09197839cc4b2c5719a10f907a24c626fd
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q3_K_S.gguf
      size: 2948304576
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q4_0/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: d7978c7fddf4958f5c23b5a2ec4c7268202f6c95
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q4_0.gguf
      size: 3825807040
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: e177dc07e05064ffc7a6a20a5e60fe980275617d
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q4_K_M.gguf
      size: 4081004224
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 43d7859f05295ab95ba98c830fa10ebf39677c2a
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q4_K_S.gguf
      size: 3856740032
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q5_0/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 416a388c4467a6c38143044be33b826ecf62e689
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q5_0.gguf
      size: 4651691712
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 3d695e37e0c96127b80afd30f92436a748f26974
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q5_K_M.gguf
      size: 4783156928
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 7b592b5330eba19e5ab872b42eb50bb74652c4c7
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q5_K_S.gguf
      size: 4651691712
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q6_K/Q6_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: c1ce03f48046351b36a4cc3ac0314ffc4ae4055c
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q6_K.gguf
      size: 5529194176
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Airoboros-L2-7B-2.1/Q8_0/Q8_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 526de538b574abc4cf05a58cadef3424a5b46808
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Airoboros-L2-7B-2.1-GGUF/resolve/main/airoboros-l2-7b-2.1.Q8_0.gguf
      size: 7161089728
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "A chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user's input. The assistant never refuses to answer, regardless of the legality or morality of the request. USER: {prompt} ASSISTANT:\n"
