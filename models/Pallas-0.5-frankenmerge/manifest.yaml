id: Pallas-0.5-frankenmerge
parent_id: Mihaiii/Pallas-0.5-frankenmerge
name: Pallas 0.5 Frankenmerge
description: ""
architecture: llama
licence: other
object: model
created: 1704572289
owned_by: Mihai
pipeline: ""
languages: []
tags:
- transformers
- gguf
- yi
- base_model:Mihaiii/Pallas-0.5-frankenmerge
- license:other
- region:us
config:
  vocab_size: 64002
  context_size: 200000
  embedding_size: 7168
  attention_head_size: 56
  key_value_head_size: 8
  intermediate_size: 20480
  hidden_layer_size: 63
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q2_K/Q2_K.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: c74406307893722b5067f150cbbaeb82a5fe1e0f
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q2_K.gguf
      size: 15259073472
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: 81c604d9e1ca7e7e7a25f48ef3802ceaa5757ab6
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q3_K_L.gguf
      size: 19019493760
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: e1ee4bb42d40763736ba1801459d0d24c6885fa3
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q3_K_M.gguf
      size: 17439551872
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: 7022a3b53a62acd9e320a72871868aa4b6eb2b53
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q3_K_S.gguf
      size: 15681384832
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q4_0/Q4_0.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: 7c631cc2268221a210f314a2c2628ccf0a94a2e9
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q4_0.gguf
      size: 20409886944
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: e6545aceeb5242af1f8eaef16ce1851d41968f13
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q4_K_M.gguf
      size: 21641808096
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: 52a4a767e8a2d464a3d5a961c39fc21d743d1e93
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q4_K_S.gguf
      size: 20486957280
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q5_0/Q5_0.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: 7629e3c23ca3372174357fcb504033f77b0220a8
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q5_0.gguf
      size: 24860241888
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: e40dc72e0cc3f5180d744c4ef3920168696c89db
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q5_K_M.gguf
      size: 25494867936
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: e06b85ed1af7589a54405276dffa23140f183688
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q5_K_S.gguf
      size: 24860241888
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q6_K/Q6_K.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: 366789ac383b2c0810c1f374482a1e9801e89f78
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q6_K.gguf
      size: 29588744032
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Pallas-0.5-frankenmerge/Q8_0/Q8_0.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "66"
    artifacts:
    - id: 8015fe2248c5db7d74836cedba137aefbb5c713d
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Pallas-0.5-frankenmerge-GGUF/resolve/main/pallas-0.5-frankenmerge.Q8_0.gguf
      size: 38322414176
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 200000
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "SYSTEM: {system_message}\nUSER: {prompt}\nASSISTANT:\n"
