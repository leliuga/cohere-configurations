id: Llama-2-13B-Chat-Dutch
parent_id: BramVanroy/Llama-2-13b-chat-dutch
name: ""
description: ""
architecture: llama
licence: cc-by-nc-sa-4.0
object: model
owned_by: Bram Vanroy
pipeline: ""
languages: []
tags:
- transformers
- gguf
- llama
- generated_from_trainer
- lora
- adapters
- nl
- dataset:BramVanroy/dutch_chat_datasets
- base_model:BramVanroy/Llama-2-13b-chat-dutch
- license:cc-by-nc-sa-4.0
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 4096
  embedding_size: 5120
  attention_head_size: 40
  key_value_head_size: 40
  intermediate_size: 13824
  hidden_layer_size: 40
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: cb2b35eb459f050d5220ba81ae773635525a9a02
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q2_K.gguf
      size: 5429348224
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 3624906006673535262a1e05dc30338186406cac
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q3_K_L.gguf
      size: 6929559424
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 3889b48f287c22f6e6e2f1e2ec87d71990c616ce
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q3_K_M.gguf
      size: 6337769344
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: c2c63d891a0ad0f68a523e6b8c9aea0dda0d472e
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q3_K_S.gguf
      size: 5658980224
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: eac25f3de83d37c0172ea5f91e13c3cb0220228a
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q4_0.gguf
      size: 7365834624
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 188254878cf0288dc225c4a15dc6303b5fcff036
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q4_K_M.gguf
      size: 7865956224
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: de922e6406f3743f8456501e176d639066328dd6
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q4_K_S.gguf
      size: 7414331264
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: c360bf786a2dc9cc6b8a84a286b490cc326dd96e
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q5_0.gguf
      size: 8972285824
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 3fd487cb755d483764924acf9ce8bdd67d6ea6e9
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q5_K_M.gguf
      size: 9229924224
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: d5e8b6dea7092921990cb2f81cc190d652226c2a
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q5_K_S.gguf
      size: 8972285824
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q6_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: a37024458234986464bee7de80437b14cd23f764
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q6_K.gguf
      size: 10679140224
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Llama-2-13B-Chat-Dutch/Q8_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: c7c9ebf69d047ea4f61c7b7e6df7cb5000a91088
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Llama-2-13B-Chat-Dutch-GGUF/resolve/main/llama-2-13b-chat-dutch.Q8_0.gguf
      size: 13831319424
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\n"
