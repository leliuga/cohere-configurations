id: deepseek-coder-33B-base
parent_id: deepseek-ai/deepseek-coder-33b-base
name: Deepseek Coder 33B Base
description: ""
architecture: llama
licence: other
object: model
owned_by: DeepSeek
pipeline: ""
languages: []
tags:
- transformers
- gguf
- deepseek
- base_model:deepseek-ai/deepseek-coder-33b-base
- license:other
- region:us
config:
  vocab_size: 32256
  context_size: 16384
  embedding_size: 7168
  attention_head_size: 56
  key_value_head_size: 8
  intermediate_size: 19200
  hidden_layer_size: 62
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q2_K
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 18546f6919b409aa5480c0bcaa39e7a179f61ff0
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q2_K.gguf
      size: 14028664800
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q3_K_L
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: d2c14b1489a0823bfe9eb274c88ef7e631707d78
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q3_K_L.gguf
      size: 17560367072
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q3_K_M
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: ac7bdbed6fae1b576d3e161e1e50f04718937286
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q3_K_M.gguf
      size: 16074928096
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q3_K_S
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 181399ad153fade8e5e907a19797cda78c4d8a9c
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q3_K_S.gguf
      size: 14421901280
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q4_0
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: aae309a833737d707e268ba4a3725c8ade118b52
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q4_0.gguf
      size: 18819440608
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q4_K_M
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 9525f7e58178b9e79cff91cb359fcc7a2297e044
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q4_K_M.gguf
      size: 19940659168
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q4_K_S
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 6719ee90ebc562dc816aa8b1d28eef0ef799fcc6
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q4_K_S.gguf
      size: 18891923424
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q5_0
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: ceb2f3fa401e83bf85405cf43d5601b34d0476f4
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q5_0.gguf
      size: 22958301152
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q5_K_M
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 95660be8a246fa186e587d893d832c124694e6d7
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q5_K_M.gguf
      size: 23535898592
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q5_K_S
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: d1d99865ef2ccbe5b39ef0f759437e91758e73b9
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q5_K_S.gguf
      size: 22958301152
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q6_K
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: cfbd12c104513126e7ae743600244081ab818768
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q6_K.gguf
      size: 27355840480
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - deepseek-coder-33B-base/Q8_0
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "65"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 61b733861f347f3854f1f22915f9a23c41e1ee6b
      uri: https://huggingface.co/TheBloke/deepseek-coder-33B-base-GGUF/resolve/main/deepseek-coder-33b-base.Q8_0.gguf
      size: 35430879200
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 16384
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "{prompt}\n"
