id: Phind-CodeLlama-34B-v1
parent_id: Phind/Phind-CodeLlama-34B-v1
name: "Phind CodeLlama 34B v1"
description: ""
architecture: llama
licence: llama2
object: model
owned_by: Phind
pipeline: ""
languages: []
tags:
- transformers
- gguf
- llama
- code llama
- base_model:Phind/Phind-CodeLlama-34B-v1
- license:llama2
- model-index
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 16384
  embedding_size: 8192
  attention_head_size: 64
  key_value_head_size: 8
  intermediate_size: 22016
  hidden_layer_size: 48
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q2_K
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 1cec59de36869ab1d1047e6686cb0f5b44a8db17
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q2_K.gguf
      size: 14210674848
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q3_K_L
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: e91c3364b7192f0d7674ab13d19bba4c5d1ecbdf
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q3_K_L.gguf
      size: 17771524256
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q3_K_M
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: a725a20a03b4c5e366bdebf411e8c410641b6932
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q3_K_M.gguf
      size: 16283594912
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q3_K_S
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 3a63248d0197f66829d40e90fb98094f7fb6d739
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q3_K_S.gguf
      size: 14605349024
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q4_0
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 190b747a3763ce2cb20c2d9f8d522d747187ef11
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q4_0.gguf
      size: 19052048544
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q4_K_M
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 1cef231c53fa2c9ae4089b322e7338577c18f25a
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q4_K_M.gguf
      size: 20219900064
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q4_K_S
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: dd51617dab534c0916447dfb505fe5b6b5c0295f
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q4_K_S.gguf
      size: 19146420384
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q5_0
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 9f601bb3993d1d535bf9018e14d4a6584c91d187
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q5_0.gguf
      size: 23237177504
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q5_K_M
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 339a1f4ecd016af30a39735cc5830c0dad65af58
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q5_K_M.gguf
      size: 23838797984
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q5_K_S
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 3a2f6506b3e5a76d1955446bea9c9753c4669572
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q5_K_S.gguf
      size: 23237177504
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q6_K
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: b3cad0840b6c85dd8a1ad63be70f8094214a7994
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q6_K.gguf
      size: 27683877024
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - Phind-CodeLlama-34B-v1/Q8_0
    - --ctx-size
    - "16384"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "51"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 3bd9616009190b3e0346bc2c4477a120e55c9110
      uri: https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q8_0.gguf
      size: 35856052384
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 16384
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "{prompt} \\n\n"
