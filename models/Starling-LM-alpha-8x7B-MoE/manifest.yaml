id: Starling-LM-alpha-8x7B-MoE
parent_id: perlthoughts/Starling-LM-alpha-8x7B-MoE
name: Starling LM Alpha 8X7B MoE
description: ""
architecture: mixtral
licence: cc-by-nc-4.0
object: model
created: 1702759375
owned_by: Ray Hernandez
pipeline: ""
languages: []
tags:
- transformers
- gguf
- mixtral
- reward model
- RLHF
- RLAIF
- en
- dataset:berkeley-nest/Nectar
- arxiv:2306.02231
- base_model:perlthoughts/Starling-LM-alpha-8x7B-MoE
- license:cc-by-nc-4.0
- text-generation-inference
- region:us
config:
  vocab_size: 32002
  context_size: 32768
  embedding_size: 4096
  attention_head_size: 32
  key_value_head_size: 8
  intermediate_size: 14336
  hidden_layer_size: 32
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Starling-LM-alpha-8x7B-MoE/Q2_K/Q2_K.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 7bf8e44f32823fc2e7875e9f0ff269e6f06d9725
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Starling-LM-alpha-8x7B-MoE-GGUF/resolve/main/starling-lm-alpha-8x7b-moe.Q2_K.gguf
      size: 15644980544
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Starling-LM-alpha-8x7B-MoE/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 897189ed489b56c4922d676dbb876ab02223cbe3
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Starling-LM-alpha-8x7B-MoE-GGUF/resolve/main/starling-lm-alpha-8x7b-moe.Q3_K_M.gguf
      size: 20364302464
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Starling-LM-alpha-8x7B-MoE/Q4_0/Q4_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 8e4957458a7be994c063747294051841c7ea6c06
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Starling-LM-alpha-8x7B-MoE-GGUF/resolve/main/starling-lm-alpha-8x7b-moe.Q4_0.gguf
      size: 26442480832
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Starling-LM-alpha-8x7B-MoE/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 741afdae5d70182e0cf2747162a19ea32118543a
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Starling-LM-alpha-8x7B-MoE-GGUF/resolve/main/starling-lm-alpha-8x7b-moe.Q4_K_M.gguf
      size: 26442480832
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Starling-LM-alpha-8x7B-MoE/Q5_0/Q5_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 9e206d9631c7e3d0ce2ad6623a8533903c4a03aa
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Starling-LM-alpha-8x7B-MoE-GGUF/resolve/main/starling-lm-alpha-8x7b-moe.Q5_0.gguf
      size: 32230228160
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Starling-LM-alpha-8x7B-MoE/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 86e6ed0161b1802048bee18360ec44160eda43ee
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Starling-LM-alpha-8x7B-MoE-GGUF/resolve/main/starling-lm-alpha-8x7b-moe.Q5_K_M.gguf
      size: 32230228160
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Starling-LM-alpha-8x7B-MoE/Q6_K/Q6_K.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: e905594713be91cc68d9aaf8b776197637fda851
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Starling-LM-alpha-8x7B-MoE-GGUF/resolve/main/starling-lm-alpha-8x7b-moe.Q6_K.gguf
      size: 38379709696
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Starling-LM-alpha-8x7B-MoE/Q8_0/Q8_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: ea8d029693152eb01d93887e6ee18fad47fdfb1e
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Starling-LM-alpha-8x7B-MoE-GGUF/resolve/main/starling-lm-alpha-8x7b-moe.Q8_0.gguf
      size: 49625216128
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 32768
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant:\n"
