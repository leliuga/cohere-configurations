id: CodeLlama-34B-Instruct
parent_id: codellama/CodeLlama-34b-instruct-hf
name: CodeLlama 34B Instruct
description: ""
architecture: llama
licence: llama2
object: model
owned_by: Meta
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- llama
- llama-2
- text-generation
- code
- arxiv:2308.12950
- base_model:codellama/CodeLlama-34b-instruct-hf
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 16384
  embedding_size: 8192
  attention_head_size: 64
  key_value_head_size: 8
  intermediate_size: 22016
  hidden_layer_size: 48
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q2_K/Q2_K.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 565e9c4b0d7121966c0ff3473a25e9c90f9c2d9b
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q2_K.gguf
      size: 14210674848
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 1a32601cc909afade19c969969409a41bc815225
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q3_K_L.gguf
      size: 17771524256
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 0fb5d6ec87f34c36b504910c37ea93b7391ce2d1
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q3_K_M.gguf
      size: 16283594912
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 8ee70e5e0dd5f93276cdeda2e8e2ec890107f3c9
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q3_K_S.gguf
      size: 14605349024
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q4_0/Q4_0.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 97fce161a84f224a7230d3bd3964f10954d0edc1
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q4_0.gguf
      size: 19052048544
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 499fbe4900d44a8cec55a826a4c9945053be521a
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q4_K_M.gguf
      size: 20219900064
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: b69fe569b4c716504eb4a954c82f15e6ce7f0a15
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q4_K_S.gguf
      size: 19146420384
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q5_0/Q5_0.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: c5d3593565633220c8b5f399dbc6d8dbfc50ec79
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q5_0.gguf
      size: 23237177504
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 912ed834c7b9c95de5f77e7cea7e612a0773e115
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q5_K_M.gguf
      size: 23838797984
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: fc3fee8b0cdf71b590029b9f504bf96f7d10fa48
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q5_K_S.gguf
      size: 23237177504
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q6_K/Q6_K.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 1210dad78bd04a7e299f1f49fec6b8cccd856c89
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q6_K.gguf
      size: 27683877024
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/CodeLlama-34B-Instruct/Q8_0/Q8_0.gguf
    - --ctx-size
    - "16384"
    - --n-gpu-layers
    - "51"
    artifacts:
    - id: 2c2b3aa28d7f3d379c1a8ec6c01af48b3f448ac3
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF/resolve/main/codellama-34b-instruct.Q8_0.gguf
      size: 35856052384
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 16384
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n{prompt}\n[/INST]\n"
