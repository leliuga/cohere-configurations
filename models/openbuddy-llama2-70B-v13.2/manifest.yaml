id: openbuddy-llama2-70B-v13.2
parent_id: OpenBuddy/openbuddy-llama2-70b-v13.2
name: OpenBuddy Llama2 70B v13.2
description: ""
architecture: llama
licence: llama2
object: model
owned_by: OpenBuddy
pipeline: ""
languages: []
tags:
- transformers
- gguf
- llama
- base_model:OpenBuddy/openbuddy-llama2-70b-v13.2
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 37632
  context_size: 4096
  embedding_size: 8192
  attention_head_size: 64
  key_value_head_size: 8
  intermediate_size: 28672
  hidden_layer_size: 80
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q2_K/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 910ee2018edf6f5166d95b0755cf43d3d04d260e
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q2_K.gguf
      size: 29332347232
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: cde494b959237e94fcea01a47bbed1e0fe622132
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q3_K_L.gguf
      size: 36205615456
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: ca8158d57f45a203a7b05a60211f4fca35eb5f4f
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q3_K_M.gguf
      size: 33244436832
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 3de85ecd449ffd574ba20eb5bebafb3dfebc6afc
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q3_K_S.gguf
      size: 29977074016
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q4_0/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 3c9e2c8b1fb7cbff4f44b6bffe276022170ad6a0
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q4_0.gguf
      size: 38936156512
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: dc1393c10da6b1fad7ecc11ee8d5e54fc970d75c
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q4_K_M.gguf
      size: 41486817632
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 4cc71c6d632cf169e8b5ec11e79b4df7c4019483
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q4_K_S.gguf
      size: 39137483104
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q5_0/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 6e928b29ff30f779860472dd048d3d25f56c1b47
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q5_0.gguf
      size: 47531071840
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: f423994f5fc78d8e598530483671a700a6883afa
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q5_K_M.gguf
      size: 48823441760
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-llama2-70B-v13.2/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 4ec267b4fefea944efaaffe68fdcc6f020e9a61e
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-llama2-70B-v13.2-GGUF/resolve/main/openbuddy-llama2-70b-v13.2.Q5_K_S.gguf
      size: 47531071840
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. You are talking to a human User.\nAlways answer as helpfully and logically as possible, while being safe. Your answers should not include any harmful, political, religious, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\nYou like to use emojis. You can speak fluently in many languages, for example: English, Chinese.\nYou cannot access the internet, but you have vast knowledge, cutoff: 2021-09.\nYou are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.\n\nUser: {prompt}\nAssistant: \n"
