id: nucleus-22B-token-500B
parent_id: NucleusAI/nucleus-22B-token-500B
name: Nucleus 22B Token 500B
description: ""
architecture: llama
licence: mit
object: model
owned_by: NucleusAI
pipeline: ""
languages: []
tags:
- transformers
- gguf
- llama
- en
- base_model:NucleusAI/nucleus-22B-token-500B
- license:mit
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 2048
  embedding_size: 6656
  attention_head_size: 52
  key_value_head_size: 52
  intermediate_size: 17920
  hidden_layer_size: 40
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q2_K
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: b00c055c856d02e7a7515bb3a9bfc57401609518
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q2_K.gguf
      size: 9083483104
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q3_K_L
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: a31b7d2f7b4cf165d57fd0efb1327a135694e741
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q3_K_L.gguf
      size: 11608836064
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q3_K_M
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: ae53b1df1aeb758759531897d969fd91e8225e6f
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q3_K_M.gguf
      size: 10610329568
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q3_K_S
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 523ff1125199a9c9dfe99baf090083c1485b886c
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q3_K_S.gguf
      size: 9465071584
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q4_0
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 78699f2bd3babce2393986d19bbf6c5c4cea071c
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q4_0.gguf
      size: 12335737824
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q4_K_M
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: f8b18cd54e12915c26d9059eb38ddb41ade1230e
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q4_K_M.gguf
      size: 13179186144
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q4_K_S
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 1c35cea3be3b6c3837e51bdf95e1851f86eb9016
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q4_K_S.gguf
      size: 12417526752
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q5_0
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: ff79e194d489ac8ee8521a6eb16f4f101f04dbe1
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q5_0.gguf
      size: 15037541344
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q5_K_M
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: c2a009ea9c7e8134e432762e89df26ba7b5232b3
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q5_K_M.gguf
      size: 15472045024
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q5_K_S
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 3a3d92f741bb1bff0451e24517f931989556f468
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q5_K_S.gguf
      size: 15037541344
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q6_K
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: a542dacb1c95f8ea488141e0b0852191058de513
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q6_K.gguf
      size: 17908207584
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - nucleus-22B-token-500B/Q8_0
    - --ctx-size
    - "2048"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 5244d7ec1fc98647e45ba4cccaa10b397a054e8b
      uri: https://huggingface.co/TheBloke/nucleus-22B-token-500B-GGUF/resolve/main/nucleus-22b-token-500b.Q8_0.gguf
      size: 23194535904
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 2048
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "{prompt}\n"
