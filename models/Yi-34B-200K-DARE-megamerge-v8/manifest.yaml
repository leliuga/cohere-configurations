id: Yi-34B-200K-DARE-megamerge-v8
parent_id: brucethemoose/Yi-34B-200K-DARE-megamerge-v8
name: Yi 34B 200K DARE MegaMerge V8
description: ""
architecture: llama
licence: other
object: model
created: 1705338406
owned_by: brucethemoose
pipeline: ""
languages: []
tags:
- transformers
- gguf
- yi
- mergekit
- merge
- Yi
- en
- arxiv:2311.03099
- arxiv:2306.01708
- base_model:brucethemoose/Yi-34B-200K-DARE-megamerge-v8
- license:other
- region:us
config:
  vocab_size: 64002
  context_size: 200000
  embedding_size: 7168
  attention_head_size: 56
  key_value_head_size: 8
  intermediate_size: 20480
  hidden_layer_size: 60
variants:
  IQ2_XS:
    dtype: IQ2_XS
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/IQ2_XS/IQ2_XS.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: ab67de2a7fc96d47cf24c599d606c919226b4c3a
      name: IQ2_XS.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.IQ2_XS.gguf
      size: 10306556160
  IQ2_XXS:
    dtype: IQ2_XXS
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/IQ2_XXS/IQ2_XXS.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: fc944a188143baf65ef0f217b04511f07267cae0
      name: IQ2_XXS.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.IQ2_XXS.gguf
      size: 9306476800
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q2_K/Q2_K.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: e81228f86f3277a99ac2b2e42b5051f219ab199b
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q2_K.gguf
      size: 12766759040
  Q2_K_S:
    dtype: Q2_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q2_K_S/Q2_K_S.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: d906cc43e0f947a14504398c2dcecc928e91750e
      name: Q2_K_S.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q2_K_S.gguf
      size: 11755210880
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 830cf362d1655a12016544e3e49b66d45fb82843
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q3_K_L.gguf
      size: 18139463232
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 1dbecd284541680b41d72f1c968debc5b0b1cda7
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q3_K_M.gguf
      size: 16654941760
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 45481f9dac8783aa65600ccbd133ff4be49ea6db
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q3_K_S.gguf
      size: 14960311872
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q4_0/Q4_0.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 75ddce7389973938ae03929d2e38cdf75d82b7c8
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q4_0.gguf
      size: 19466548640
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: e58d35140936bd8017a555332da371eaea978b40
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q4_K_M.gguf
      size: 20658730400
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 6da00d566ef1f21b284731d778412d1b899fa177
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q4_K_S.gguf
      size: 19598669216
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q5_0/Q5_0.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 5c8d49886b6899ed48a7380a92c3b4b89b582713
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q5_0.gguf
      size: 23707712672
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 09ece5ac7319007b89f52848ad1225afbe08d85b
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q5_K_M.gguf
      size: 24321866912
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 7c362648cf0a1ea664878ba68af83c8a7480c16d
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q5_K_S.gguf
      size: 23707712672
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q6_K/Q6_K.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: 96eef91ca6b8ae14f809a15963a8b42762889c66
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q6_K.gguf
      size: 28213949472
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Yi-34B-200K-DARE-megamerge-v8/Q8_0/Q8_0.gguf
    - --ctx-size
    - "200000"
    - --n-gpu-layers
    - "63"
    artifacts:
    - id: ee0ab2f903d2ee96fd71128610c11edf4fccfce2
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF/resolve/main/yi-34b-200k-dare-megamerge-v8.Q8_0.gguf
      size: 36542312224
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 200000
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "SYSTEM: {system_message}\nUSER: {prompt}\nASSISTANT:\n"
