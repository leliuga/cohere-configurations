id: dolphin-2.6-mistral-7B-dpo-laser
parent_id: cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser
name: Dolphin 2.6 Mistral 7B DPO Laser
description: ""
architecture: mistral
licence: apache-2.0
object: model
owned_by: Cognitive Computations
pipeline: ""
languages: []
tags:
- transformers
- gguf
- mistral
- en
- dataset:ehartford/dolphin
- dataset:jondurbin/airoboros-2.2.1
- dataset:ehartford/dolphin-coder
- dataset:teknium/openhermes
- dataset:ise-uiuc/Magicoder-OSS-Instruct-75K
- dataset:ise-uiuc/Magicoder-Evol-Instruct-110K
- dataset:LDJnr/Capybara
- arxiv:2312.13558
- base_model:cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser
- license:apache-2.0
- text-generation-inference
- region:us
config:
  vocab_size: 32001
  context_size: 32768
  embedding_size: 4096
  attention_head_size: 32
  key_value_head_size: 8
  intermediate_size: 14336
  hidden_layer_size: 32
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q2_K/Q2_K.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: ecd59748b202df5669f8cf4635c1aa951219950d
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q2_K.gguf
      size: 3083102624
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 0232461677f752ac6cb2d7fa17eca565b6cde259
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q3_K_L.gguf
      size: 3822029632
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 4ba3d9e94f8b6435021c67cf2e1236039a70b9c3
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q3_K_M.gguf
      size: 3518991168
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 95704e3c4a7e64b4537701155231a9bc5d0af8e9
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q3_K_S.gguf
      size: 3164572480
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q4_0/Q4_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: bfbacee3ecac9cbd0585635bce4c737b0c26fd1c
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q4_0.gguf
      size: 4108922208
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 9ba1085db2c6c0303833c1a74835622b79902218
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q4_K_M.gguf
      size: 4368444768
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 8dec8cf9da4430531a09e127732dbc41b77417ba
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q4_K_S.gguf
      size: 4140379488
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q5_0/Q5_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 3b1c0b0bf01a3a80476c693d0e008b6185688be8
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q5_0.gguf
      size: 4997721952
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 353dd8aada72ba2aadd0cf3ed1ff5c96a7e038ea
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q5_K_M.gguf
      size: 5131415392
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 9492171fbf4652a1c5965f4c17b5bec060fa7cae
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q5_K_S.gguf
      size: 4997721952
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q6_K/Q6_K.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 8ab720ace98c76cdbeaf0fd5a1e8a296960d2ac5
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q6_K.gguf
      size: 5942071680
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/dolphin-2.6-mistral-7B-dpo-laser/Q8_0/Q8_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: f7c69476c8b431fb189c457d07c59f17db1594a5
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF/resolve/main/dolphin-2.6-mistral-7b-dpo-laser.Q8_0.gguf
      size: 7695866176
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 32768
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: |
        <|im_start|>system
        {system_message}<|im_end|>
        <|im_start|>user
        {prompt}<|im_end|>
        <|im_start|>assistant
