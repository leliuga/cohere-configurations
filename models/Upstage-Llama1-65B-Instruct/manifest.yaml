id: Upstage-Llama1-65B-Instruct
parent_id: upstage/llama-65b-instruct
name: Llama 65B Instruct
description: ""
architecture: llama
licence: other
object: model
created: 1695168695
owned_by: upstage
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- llama
- upstage
- instruct
- instruction
- text-generation
- en
- base_model:upstage/llama-65b-instruct
- license:other
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 2048
  embedding_size: 8192
  attention_head_size: 64
  key_value_head_size: 0
  intermediate_size: 22016
  hidden_layer_size: 80
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q2_K/Q2_K.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: daec148e601aa6b56cf8b9f9eb2ba9e0f1dde3fa
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q2_K.gguf
      size: 27043165120
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: 14a73f992d2a86a617994590880cd4c2c372a987
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q3_K_L.gguf
      size: 34648372160
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: 7f0b5b53a2570866b1c6063736ffedeccfeb4d5d
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q3_K_M.gguf
      size: 31564510144
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: f212a455c3ba8d07014340a55b711732d2258895
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q3_K_S.gguf
      size: 28160308160
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q4_0/Q4_0.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: 711a282133ca022774b18420993350e6a4b5ba78
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q4_0.gguf
      size: 36796068800
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: 2bf9e249dfe384a5b3708777c6a4ed3b4aad43e5
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q4_K_M.gguf
      size: 39348040640
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: 7b7f3cc12d3db2b959f3b93a73e04de785d4e0fc
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q4_K_S.gguf
      size: 36919800768
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q5_0/Q5_0.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: 0a553ffab5c5a2a70460f3313035187d64f35413
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q5_0.gguf
      size: 44923843520
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: 0d40a3197d07b8c1a2a477b86880d172ad79e91b
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q5_K_M.gguf
      size: 46238495680
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Upstage-Llama1-65B-Instruct/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "2048"
    - --n-gpu-layers
    - "83"
    artifacts:
    - id: 6821ae67db785ff7145eeebaf108b8469c53cd12
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGUF/resolve/main/upstage-llama-65b-instruct.Q5_K_S.gguf
      size: 44923843520
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 2048
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "### System:\n{system_message}\n\n### User:\n{prompt}\n\n### Assistant:\n"
