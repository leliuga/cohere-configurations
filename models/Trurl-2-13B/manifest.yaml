id: Trurl-2-13B
parent_id: Voicelab/trurl-2-13b
name: Trurl 2 13B
description: ""
architecture: llama
licence: llama2
object: model
owned_by: Voicelab
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- llama
- voicelab
- pytorch
- llama-2
- trurl
- trurl-2
- text-generation
- en
- pl
- base_model:Voicelab/trurl-2-13b
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 32001
  context_size: 4096
  embedding_size: 5120
  attention_head_size: 40
  key_value_head_size: 40
  intermediate_size: 13824
  hidden_layer_size: 40
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q2_K/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: f289d032e00b62e4565dd519223c91b46829600f
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q2_K.gguf
      size: 5429354176
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 11bc8c1f3002556c8a5b31f6cde6a176e275887a
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q3_K_L.gguf
      size: 6929565888
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: f197de8a0cc717ae7a17674e22513d152d5de76a
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q3_K_M.gguf
      size: 6337775808
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 491ef3ca93452dc7f1744219d3945f88c64f7332
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q3_K_S.gguf
      size: 5658986688
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q4_0/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 662a5b4858751544f71196049478f2e0567320d7
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q4_0.gguf
      size: 7365841760
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 12228aec7dced2256f1a324fab4be69dec747aa1
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q4_K_M.gguf
      size: 7865963360
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 41a28e24cff1578ab753948cad8cd18130dbcfd2
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q4_K_S.gguf
      size: 7414338400
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q5_0/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 284600583581b5fe7ae86e8f6b401d3ebb3c5b8c
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q5_0.gguf
      size: 8972293600
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 7469b47f0e45b426b49b6a37d4407b3ffda4ebdf
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q5_K_M.gguf
      size: 9229932000
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 2b058d1014785475b56213cae451d273d90a21a2
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q5_K_S.gguf
      size: 8972293600
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q6_K/Q6_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 8f705a48e76f91a4244967b089c0ae170069fa8d
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q6_K.gguf
      size: 10679148704
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-13B/Q8_0/Q8_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "16384"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 1eba5d6619fe2b629555d1bcce0845bce752ac67
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-13B-GGUF/resolve/main/trurl-2-13b.Q8_0.gguf
      size: 13831330336
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\n"
