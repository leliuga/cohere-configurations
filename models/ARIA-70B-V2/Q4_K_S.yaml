id: ARIA-70B-V2/Q4_K_S
parent_id: Faradaylab/ARIA-70B-V2
name: ARIA 70B V2
description: ""
architecture: llama
licence: llama2
object: model
owned_by: Faradaylab
artifact: https://huggingface.co/TheBloke/ARIA-70B-V2-GGUF/resolve/main/aria-70b-v2.Q4_K_S.gguf
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- llama
- code
- text-generation-inference
- "Meta "
- facebook
- pytorch
- openassistant
- data
- education
- languages
- text-generation
- fr
- en
- arxiv:2307.09288
- base_model:Faradaylab/ARIA-70B-V2
- license:llama2
- region:us
dtype: Q4_K_S
file_size: 39073575840
params_size: ""
vocab_size: 32000
context_size: 4096
embedding_size: 8192
pretrained: false
finetuned: false
quantized: true
backend: llama-backend
chat_options:
  cache_prompt: true
  frequency_penalty: 0.0
  grammar: ""
  ignore_eos: false
  mirostat: 0
  mirostat_eta: 0.1
  mirostat_tau: 5.0
  n_keep: -1
  n_predict: -1
  n_probs: 0
  penalize_nl: true
  presence_penalty: 0.0
  repeat_last_n: 64
  repeat_penalty: 1.1
  seed: -1
  temperature: 0.8
  tfs_z: 1.0
  top_k: 40
  top_p: 0.95
  typical_p: 1.0
prompt:
  system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
  template: "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\n"
