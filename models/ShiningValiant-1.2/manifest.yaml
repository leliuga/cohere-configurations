id: ShiningValiant-1.2
parent_id: ValiantLabs/ShiningValiant
name: ShiningValiant 1.2
description: ""
architecture: llama
licence: llama2
object: model
owned_by: Valiant Labs
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- llama
- shining-valiant
- valiant
- valiant-labs
- llama-2
- llama-2-chat
- 70b
- text-generation
- en
- base_model:ValiantLabs/ShiningValiant
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 4096
  embedding_size: 8192
  attention_head_size: 64
  key_value_head_size: 8
  intermediate_size: 28672
  hidden_layer_size: 80
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: c83c04529319ed09d512e3047d2ff4ee3ae8f3c7
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q2_K.gguf
      size: 29279253504
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: a0b52a2fbe788685dac95c77a1fe04a77026a1cf
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q3_K_L.gguf
      size: 36147835904
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: 3efd14781740ee212065a150d45174202398bd14
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q3_K_M.gguf
      size: 33186657280
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: 493fdf1a8616bd38289798c7dae994bde692ae83
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q3_K_S.gguf
      size: 29919294464
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: 5ee16de969a1c56061a0e1cc1ae1636f8eec6c5a
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q4_0.gguf
      size: 38872249344
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: 9f71bce6a017ff8be277ff0935cec3efe7b2d963
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q4_K_M.gguf
      size: 41422910464
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: 95c9392c4630e4ded5576b399cc33d22cb01d514
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q4_K_S.gguf
      size: 39073575936
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: 9ac81cd32593177473d8425baa226a4d5edf63aa
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q5_0.gguf
      size: 47461397504
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: 382715391f1e173f57b3735c2cebe4d92920caed
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q5_K_M.gguf
      size: 48753767424
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/ShiningValiant-1.2/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "83"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifacts:
    - id: 89a0258aef7a04bf913c24133660d57923c82360
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/ShiningValiant-1.2-GGUF/resolve/main/shiningvaliant-1.2.Q5_K_S.gguf
      size: 47461397504
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\n"
