---
license: other
tasks: 
- code-generation
---
# Model Card for CodeFuse-CodeLlama-34B
![logo](LOGO.png)

[[ä¸­æ–‡]](#chinese)    [[English]](#english)



<a id="english"></a>

## Model Description

CodeFuse-CodeLlama-34B is a 34B Code-LLM finetuned by QLoRA of multiple code tasksï¼ˆ600k instrunctions/answersï¼‰ on the base model CodeLlama-34b-Python. 
The context length of finetuning is 4K while it is able to be finetuned by 16k context if necessary.
<br>

## News and Updates

ğŸ”¥ğŸ”¥ğŸ”¥ 2023-09-26 We are pleased to announce the release of the [4-bit quantized version](https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B-4bits) of CodeFuse-CodeLlama-34B. Despite the quantization process, the model still achieves a remarkable 73.8% accuracy (greedy decoding) on the HumanEval pass@1 metric.

ğŸ”¥ğŸ”¥ğŸ”¥ 2023-09-11 CodeFuse-CodeLlama34B has achieved 74.4% of pass@1 (greedy decoding) on HumanEval, which is SOTA results for openspurced LLMs at present.

<br>

## Code Community

**Homepage**: ğŸ¡ https://github.com/codefuse-ai (**Please give us your support with a StarğŸŒŸ + ForkğŸš€ + WatchğŸ‘€**)

+ If you wish to fine-tune the model yourself, you can visit âœ¨[MFTCoder](https://github.com/codefuse-ai/MFTCoder)âœ¨âœ¨

+ If you wish to deploy the model yourself, you can visit âœ¨[FasterTransformer4CodeFuse](https://github.com/codefuse-ai/FasterTransformer4CodeFuse)âœ¨âœ¨

+ If you wish to see a demo of the model, you can visit âœ¨[CodeFuse Demo](https://github.com/codefuse-ai/codefuse)âœ¨âœ¨


## Performance


| Model                       | HumanEval(pass@1) |  Date   |
|:----------------------------|:-----------------:|:-------:|
| **CodeFuse-CodeLlama-34B**  |     **74.4%**      | 2023.9  |
| WizardCoder-Python-34B-V1.0 |       73.2%       | 2023.8  |
| GPT-4(zero-shot)            |       67.0%       | 2023.3  |
| PanGu-Coder2 15B            |       61.6%       | 2023.8  |
| CodeLlama-34b-Python        |       53.7%       | 2023.8  |
| CodeLlama-34b               |       48.8%       | 2023.8  |
| GPT-3.5(zero-shot)          |       48.1%       | 2022.11 |
| OctoCoder                   |       46.2%       | 2023.8  |
| StarCoder-15B               |       33.6%       | 2023.5  |
| LLaMA 2 70B(zero-shot)      |       29.9%       | 2023.7  |

<br>

## Requirements

* python>=3.8 
* pytorch>=2.0.0
* transformers==4.32.0
* Sentencepiece
* CUDA 11.4
  <br>

##  Inference String Format

The inference string is a concatenated string formed by combining conversation data(system, human and bot contents) in the training data format.  It is used as input during the inference process.
Here is an example format of the concatenated string:

```python
"""
<|role_start|>system<|role_end|>System instruction
<|role_start|>human<|role_end|>Human 1st round input
<|role_start|>bot<|role_end|>Bot 1st round output</s>
<|role_start|>human<|role_end|>Human 2nd round input
<|role_start|>bot<|role_end|>Bot 2nd round output</s>
...
...
...
<|role_start|>human<|role_end|>Human nth round input
<|role_start|>bot<|role_end|>{Bot output to be genreated}</s>
"""
```

When applying inference, you always make your input string end with "<|role_start|>bot<|role_end|>" to ask the model generating answers.

## Quickstart

```bash
pip install -r requirements.txt
```

```python
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
)
tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)
tokenizer.padding_side = "left"
tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<unk>")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("</s>")
# try 4bit loading if cuda memory not enough
model = AutoModelForCausalLM.from_pretrained(mode_name_or_path,
                                             trust_remote_code=True,
                                             load_in_4bit=False,
                                             device_map="auto",
                                             torch_dtype=torch.bfloat16)
model.eval()

HUMAN_ROLE_START_TAG = "<|role_start|>human<|role_end|>"
BOT_ROLE_START_TAG = "<|role_start|>bot<|role_end|>"

text = f"{HUMAN_ROLE_START_TAG}write a python function of quick sort.{BOT_ROLE_START_TAG}" 
inputs = tokenizer(text, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
```

## MD5
We notice that the file may be corrupted during transfer process. Please check MD5 value before use.

| Model File                       | MD5 Value                        |
|:---------------------------------|:--------------------------------:|
| pytorch_model-00001-of-00007.bin | 8d544b1bcb3449934184d4141137329c |
| pytorch_model-00002-of-00007.bin | 9d5dbb30911e48a42fb6d0fcabb322a4 |
| pytorch_model-00003-of-00007.bin | b0d4aecee0457d9332005a187e1fffed |
| pytorch_model-00004-of-00007.bin | 5c7e002de5eab77d0194a2b0f6de0c24 |
| pytorch_model-00005-of-00007.bin | d22a511aa26b5b17117b665a877490ab |
| pytorch_model-00006-of-00007.bin | a5c28ac277fac07d16dd66537e54d109 |
| pytorch_model-00007-of-00007.bin | a967e2c6195477b7407089c0bffa2d53 |


## Citation
If you find our [work](https://arxiv.org/abs/2311.02303) useful or helpful for your R&D works, please feel free to cite our paper as below.
```
@article{mftcoder2023,
      title={MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning}, 
      author={Bingchang Liu and Chaoyu Chen and Cong Liao and Zi Gong and Huan Wang and Zhichao Lei and Ming Liang and Dajun Chen and Min Shen and Hailian Zhou and Hang Yu and Jianguo Li},
      year={2023},
      journal={arXiv preprint arXiv},
      archivePrefix={arXiv},
      eprint={2311.02303}
}
```

<a id="chinese"></a>

## æ¨¡å‹ç®€ä»‹

CodeFuse-CodeLlama34B-MFT æ˜¯ä¸€ä¸ªé€šè¿‡QLoRAå¯¹åŸºåº§æ¨¡å‹CodeLlama-34b-Pythonè¿›è¡Œå¤šä»£ç ä»»åŠ¡å¾®è°ƒçš„ä»£ç å¤§æ¨¡å‹ã€‚æ¨¡å‹å¾®è°ƒé‡‡ç”¨äº†4kä¸Šä¸‹æ–‡ã€‚å¦‚æœæœ‰å¿…è¦ï¼Œå¯ä»¥æ‰©å±•åˆ°16kã€‚
<br>

## æ–°é—»

ğŸ”¥ğŸ”¥ğŸ”¥ CodeFuse-CodeLlama34B-MFTæ¨¡å‹åœ¨HumanEval pass@1ä¸Šå¯ä»¥è¾¾åˆ°74.4%, ä¸ºå½“å‰å¼€æºSOTAã€‚

<br>

## ä»£ç ç¤¾åŒº
**å¤§æœ¬è¥**ï¼š ğŸ¡ https://github.com/codefuse-ai ï¼ˆ**æ¬¢è¿ä¸ºæˆ‘ä»¬çš„é¡¹ç›®ä¸€é”®ä¸‰è¿ StarğŸŒŸÂ + ForkğŸš€ + WatchğŸ‘€**ï¼‰

+ å¦‚æœæ‚¨æƒ³è‡ªå·±å¾®è°ƒè¯¥æ¨¡å‹ï¼Œå¯ä»¥è®¿é—® âœ¨[MFTCoder](https://github.com/codefuse-ai/MFTCoder)âœ¨âœ¨

+ å¦‚æœæ‚¨æƒ³è‡ªå·±éƒ¨ç½²è¯¥æ¨¡å‹ï¼Œå¯ä»¥è®¿é—® âœ¨[FasterTransformer4CodeFuse](https://github.com/codefuse-ai/FasterTransformer4CodeFuse)âœ¨âœ¨

+ å¦‚æœæ‚¨æƒ³è§‚çœ‹è¯¥æ¨¡å‹ç¤ºä¾‹ï¼Œå¯ä»¥è®¿é—® âœ¨[CodeFuse Demo](https://github.com/codefuse-ai/codefuse)âœ¨âœ¨


## è¯„æµ‹è¡¨ç°(ä»£ç )

| æ¨¡å‹                          | HumanEval(pass@1) |   æ—¥æœŸ    |
|:----------------------------|:-----------------:|:-------:|
| **CodeFuse-CodeLlama-34B**  |     **74.4%**      | 2023.9  |
| WizardCoder-Python-34B-V1.0 |       73.2%       | 2023.8  |
| GPT-4(zero-shot)            |       67.0%       | 2023.3  |
| PanGu-Coder2 15B            |       61.6%       | 2023.8  |
| CodeLlama-34b-Python        |       53.7%       | 2023.8  |
| CodeLlama-34b               |       48.8%       | 2023.8  |
| GPT-3.5(zero-shot)          |       48.1%       | 2022.11 |
| OctoCoder                   |       46.2%       | 2023.8  |
| StarCoder-15B               |       33.6%       | 2023.5  |
| LLaMA 2 70B(zero-shot)      |       29.9%       | 2023.7  |

<br>

## Requirements

* python>=3.8 
* pytorch>=2.0.0
* transformers==4.32.0
* CUDA 11.4
<br>

## æ¨ç†æ•°æ®æ ¼å¼

æ¨ç†æ•°æ®ä¸ºæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®æ ¼å¼ä¸‹æ‹¼æ¥çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œå®ƒä¹Ÿæ˜¯æ¨ç†æ—¶è¾“å…¥promptæ‹¼æ¥çš„æ–¹å¼ï¼š

```python
"""
<|role_start|>system<|role_end|>è¿™æ˜¯SystemæŒ‡ä»¤
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬1è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>è¿™æ˜¯ç¬¬1è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹</s>
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬2è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>è¿™æ˜¯ç¬¬2è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹</s>
...
...
...
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬nè½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>{æ¨¡å‹ç°åœ¨è¦ç”Ÿæˆçš„å†…å®¹}</s>
"""
```

æ¨ç†æ—¶ï¼Œè¯·ç¡®ä¿æ‹¼æ¥çš„promptå­—ç¬¦ä¸²ä»¥"<|role_start|>bot<|role_end|>"ç»“å°¾ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚

## å¿«é€Ÿä½¿ç”¨

```python
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
)
tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)
tokenizer.padding_side = "left"
tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<unk>")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("</s>")
# å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¯ä»¥è€ƒè™‘é‡åŒ–åŠ è½½
model = AutoModelForCausalLM.from_pretrained(mode_name_or_path,
                                             trust_remote_code=True,
                                             load_in_4bit=False,
                                             device_map="auto",
                                             torch_dtype=torch.bfloat16)
model.eval()

HUMAN_ROLE_START_TAG = "<|role_start|>human<|role_end|>"
BOT_ROLE_START_TAG = "<|role_start|>bot<|role_end|>"

text = f"{HUMAN_ROLE_START_TAG}è¯·ç”¨C++å®ç°æ±‚è§£ç¬¬nä¸ªæ–æ³¢é‚£å¥‘æ•°{BOT_ROLE_START_TAG}" 
inputs = tokenizer(text, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
```


## MD5
æˆ‘ä»¬å‘ç°æ¨¡å‹æ–‡ä»¶å¯èƒ½ä¼šåœ¨ä¼ è¾“è¿‡ç¨‹ä¸­æŸåï¼Œä½¿ç”¨å‰è¯·æ£€æŸ¥æ–‡ä»¶MD5å€¼ã€‚

| æ¨¡å‹æ–‡ä»¶                           | MD5å€¼                            |
|:---------------------------------|:--------------------------------:|
| pytorch_model-00001-of-00007.bin | 8d544b1bcb3449934184d4141137329c |
| pytorch_model-00002-of-00007.bin | 9d5dbb30911e48a42fb6d0fcabb322a4 |
| pytorch_model-00003-of-00007.bin | b0d4aecee0457d9332005a187e1fffed |
| pytorch_model-00004-of-00007.bin | 5c7e002de5eab77d0194a2b0f6de0c24 |
| pytorch_model-00005-of-00007.bin | d22a511aa26b5b17117b665a877490ab |
| pytorch_model-00006-of-00007.bin | a5c28ac277fac07d16dd66537e54d109 |
| pytorch_model-00007-of-00007.bin | a967e2c6195477b7407089c0bffa2d53 |