id: yayi2-30B-llama
parent_id: cognitivecomputations/yayi2-30b-llama
name: Yayi2 30B Llama
description: ""
architecture: llama
licence: other
object: model
owned_by: Cognitive Computations
pipeline: ""
languages: []
tags:
- transformers
- gguf
- yayi2
- zh
- en
- arxiv:2307.09288
- base_model:cognitivecomputations/yayi2-30b-llama
- license:other
- region:us
config:
  vocab_size: 81920
  context_size: 4096
  embedding_size: 7168
  attention_head_size: 64
  key_value_head_size: 1
  intermediate_size: 16384
  hidden_layer_size: 64
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q2_K/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 54932e3c4de4f85133080567ef8a39ee2a4ae763
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q2_K.gguf
      size: 12899685024
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 45d02373ade727241540c73e46db6752ac8671d6
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q3_K_L.gguf
      size: 16097301152
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 3a14dd08799f227f77a6d7b8933078b6b8616882
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q3_K_M.gguf
      size: 14769873568
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 41659a3d111c0710b87b294b4e1c8599b3252c9f
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q3_K_S.gguf
      size: 13298512544
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q4_0/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 38128d3069c4fe27d7dfa0be3aeca34ddd4387ec
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q4_0.gguf
      size: 17258345120
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 8592cdf6e752e6209de521febdceb6c9dde28e5a
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q4_K_M.gguf
      size: 18233852576
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: a7bb481ec2d2f64f4b78474efed579fbd88d5f26
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q4_K_S.gguf
      size: 17317466784
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q5_0/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 7042160d26e377286e686960f7aaf3e41b994c2c
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q5_0.gguf
      size: 20985246368
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: d799d378ccbc8eb8f84e05d189f31dcab7861a9e
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q5_K_M.gguf
      size: 21487780512
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 52fed7966cfbc31a38a5d9cce941af0eb1ce1dae
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q5_K_S.gguf
      size: 20985246368
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q6_K/Q6_K.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: c31e2ad50556436eefa451dcfc30b4d57363b59a
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q6_K.gguf
      size: 24945078944
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/yayi2-30B-llama/Q8_0/Q8_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "67"
    artifacts:
    - id: 0b0410e6aaaa4f185557e57b07448343033bc5be
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/yayi2-30B-llama-GGUF/resolve/main/yayi2-30b-llama.Q8_0.gguf
      size: 32308163232
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "{prompt}\n"
