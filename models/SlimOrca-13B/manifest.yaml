id: SlimOrca-13B
parent_id: ajibawa-2023/SlimOrca-13B
name: SlimOrca 13B
description: ""
architecture: llama
licence: cc-by-nc-nd-4.0
object: model
created: 1701379568
owned_by: Feynman Innovations
pipeline: ""
languages: []
tags:
- transformers
- gguf
- llama
- en
- dataset:Open-Orca/SlimOrca
- dataset:ajibawa-2023/SlimOrca-ShareGPT
- base_model:ajibawa-2023/SlimOrca-13B
- license:cc-by-nc-nd-4.0
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 4096
  embedding_size: 5120
  attention_head_size: 40
  key_value_head_size: 40
  intermediate_size: 13824
  hidden_layer_size: 40
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q2_K/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 851848772e387c2e15f02c60f850c4b22ff7284f
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q2_K.gguf
      size: 5429348352
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 7865f3cea5181cefc533e95293ec32e501a25185
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q3_K_L.gguf
      size: 6929559552
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: a85e926772e0a21d8917367f0feb62a67c4e4e45
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q3_K_M.gguf
      size: 6337769472
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: df5fa17d56e3b4057e55ef55f294d871a11ab77d
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q3_K_S.gguf
      size: 5658980352
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q4_0/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 1477f835460684c27111ad93d9fa0b6c641e970c
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q4_0.gguf
      size: 7365834752
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 69e53f5e4852b3eebe7061aef47c5d3788bbaad0
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q4_K_M.gguf
      size: 7865956352
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: ccd31d2e2b00983130522e3e936e65744848ff1a
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q4_K_S.gguf
      size: 7414331392
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q5_0/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 787c17c167d61423c3c308c1fb86d14fdfaf62e4
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q5_0.gguf
      size: 8972285952
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 35517ff04bc603f4f0bbde7bbaff20a71d498224
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q5_K_M.gguf
      size: 9229924352
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: e30933c763c49dc80b0017ab0023bdd835dda27f
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q5_K_S.gguf
      size: 8972285952
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q6_K/Q6_K.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 394b6057b37f9158c41de20299915088f7ba3f26
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q6_K.gguf
      size: 10679140352
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/SlimOrca-13B/Q8_0/Q8_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 2fbc628c6c7b1b301b89a7b9db19c76834a8117a
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/SlimOrca-13B-GGUF/resolve/main/slimorca-13b.Q8_0.gguf
      size: 13831319552
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "This is a conversation with your Assistant. It is a computer program designed to help you with various tasks such as answering questions, providing recommendations, and helping with decision making. You can ask it anything you want and it will do its best to give you accurate and relevant information.\n\nContext\nYou are a helpful AI assistant.\n\nUSER: {prompt}\nASSISTANT:\n"
