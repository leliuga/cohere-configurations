id: NeuralQuant-9B
parent_id: mlabonne/NeuralQuant-9B
name: NeuralQuant 9B
description: ""
architecture: mistral
licence: apache-2.0
object: model
created: 1703869583
owned_by: Maxime Labonne
pipeline: ""
languages: []
tags:
- transformers
- gguf
- mistral
- merge
- base_model:mlabonne/NeuralQuant-9B
- license:apache-2.0
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 32768
  embedding_size: 4096
  attention_head_size: 32
  key_value_head_size: 8
  intermediate_size: 14336
  hidden_layer_size: 40
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q2_K/Q2_K.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 12096ec0e7ec6a2999b075545fe3bc7aebd64d09
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q2_K.gguf
      size: 3816999744
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: f91c0fb2d18e366cf524bda23d0995cd3abd3ce0
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q3_K_L.gguf
      size: 4737329984
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 382382e513ac7da000abe36eca657e9e1d4a1b2e
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q3_K_M.gguf
      size: 4354599744
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: d96508c9f808083d3e9f15667abab7613659bb87
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q3_K_S.gguf
      size: 3915508544
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q4_0/Q4_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 9288db4335da8f1e19f5527f601a4c7f6fbaf3a8
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q4_0.gguf
      size: 5091593024
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 2fb59a8f7ba073dc96e6f8afdac3a1cd3ec54eb1
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q4_K_M.gguf
      size: 5415996224
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 97e5c371d004561c5d843b0fa3b2a9a7617e2bb7
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q4_K_S.gguf
      size: 5123050304
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q5_0/Q5_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 766f37cc8329b2e417101afac57ddbd1fd4fa334
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q5_0.gguf
      size: 6198496064
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: d2651e57b165969a7b69ca76d797b11e52754802
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q5_K_M.gguf
      size: 6365612864
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 4c6ec7f79c97f9f18808a5d5b713f736f821a055
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q5_K_S.gguf
      size: 6198496064
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q6_K/Q6_K.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 82b26a06b7d1f42cc69c6c9bd47ae6193799f749
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q6_K.gguf
      size: 7374580544
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/NeuralQuant-9B/Q8_0/Q8_0.gguf
    - --ctx-size
    - "32768"
    - --n-gpu-layers
    - "43"
    artifacts:
    - id: 042126a5c1ec95c93f0df2575197a57c0b690f5b
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/NeuralQuant-9B-GGUF/resolve/main/neuralquant-9b.Q8_0.gguf
      size: 9550951168
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 32768
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "{prompt}\n"
