id: Trurl-2-7B
parent_id: Voicelab/trurl-2-7b
name: Trurl 2 7B
description: ""
architecture: llama
licence: llama2
object: model
owned_by: Voicelab
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- llama
- voicelab
- pytorch
- llama-2
- trurl
- trurl-2
- text-generation
- en
- pl
- base_model:Voicelab/trurl-2-7b
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 32001
  context_size: 4096
  embedding_size: 4096
  attention_head_size: 32
  key_value_head_size: 32
  intermediate_size: 11008
  hidden_layer_size: 32
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 4567adeac39684f72032da465e13338c2bd64fd6
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q2_K.gguf
      size: 2825945408
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 497500cdea7eb294e297c759c9b18de028e86ad7
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q3_K_L.gguf
      size: 3597116128
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 17791089dda7fd6b0726e1b81fa0c028cc576da5
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q3_K_M.gguf
      size: 3298009824
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: b3562d1f8af7f4f76cc1b1b4b5820d3e0dd68111
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q3_K_S.gguf
      size: 2948309728
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: d347b6ec9b0058b5851d0cb5e4d9d729c20c4a4d
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q4_0.gguf
      size: 3825812736
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 9f6cb2c6ce7e2306d802af343f4606d1c6d50c89
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q4_K_M.gguf
      size: 4081009920
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 001bb424097166df8868f424694852ccf0087ee1
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q4_K_S.gguf
      size: 3856745728
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: e1ebb877a8c2e0833cf75de4ff29f49107d685be
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q5_0.gguf
      size: 4651697920
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: a000f1633390d458c8bb03f0d172edf016ed69ba
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q5_K_M.gguf
      size: 4783163136
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 566a6229b125e1b14a3008cc0c02d538076b90b7
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q5_K_S.gguf
      size: 4651697920
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q6_K.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 57443f6def90e935e055cf2ba87d2601a94d1e2b
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q6_K.gguf
      size: 5529200928
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Trurl-2-7B/Q8_0.gguf
    - --ctx-size
    - "4096"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 1e68f8bd4ea0e3d7c74ce3caad730b5e3209e7fb
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Trurl-2-7B-GGUF/resolve/main/trurl-2-7b.Q8_0.gguf
      size: 7161098464
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\n"
