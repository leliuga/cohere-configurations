---
license: apache-2.0
language:
- en
---

# Marcoroni-7B-v3

<img src="https://cdn-uploads.huggingface.co/production/uploads/637aebed7ce76c3b834cea37/20uN0wMu2zTyVGgXV9PIo.png"  width = 60%>

# Updates 
December 11, 2023:
Marcoroni-7B-v3 has placed **#5** overall and **#1** for 7 billion parameter models on the [Hugging Face Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)!


# Model Details
* **Trained by**: trained by AIDC AI-Business.
* **Model type:**  **Marcoroni-7B-v3** is an auto-regressive language model based on mistralai/Mistral-7B-v0.1.
* **Language(s)**: English

This is a DPO fine tuned model of [Q-bert/MetaMath-Cybertron-Starling](https://huggingface.co/Q-bert/MetaMath-Cybertron-Starling).

We fine-tuned using 32k data generated by GPT-4 and other models.

# Prompting

## Prompt Template for alpaca style

```
### Instruction:

<prompt> (without the <>)

### Response:
```