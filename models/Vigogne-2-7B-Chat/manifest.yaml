id: Vigogne-2-7B-Chat
parent_id: bofenghuang/vigogne-2-7b-chat
name: Vigogne 2 7B Chat
description: ""
architecture: llama
licence: llama2
object: model
created: 1693934312
owned_by: bofenghuang
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- llama
- LLM
- llama-2
- text-generation
- fr
- base_model:bofenghuang/vigogne-2-7b-chat
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 32000
  context_size: 4096
  embedding_size: 4096
  attention_head_size: 32
  key_value_head_size: 32
  intermediate_size: 11008
  hidden_layer_size: 32
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q2_K/Q2_K.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 40d15d149800aedcea8207d430545431fba46a09
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q2_K.gguf
      size: 2825940672
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 86bce835e14c3de1886e19ca0150bb0e27ec9b67
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q3_K_L.gguf
      size: 3597110976
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 5523a5f5396452c33b045319ec663c60558b0dd6
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q3_K_M.gguf
      size: 3298004672
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 7052b42db4e64cadfd3a0eab99016fd9e9b6b8db
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q3_K_S.gguf
      size: 2948304576
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q4_0/Q4_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: b21e7650dc81720a01c3580611dfc010d8647577
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q4_0.gguf
      size: 3825807040
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 0c2901b52561e58178a5223496388d6c6234f50f
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q4_K_M.gguf
      size: 4081004224
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: deeaf50090463a2f1b7a6205fa24d8dce01ea209
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q4_K_S.gguf
      size: 3856740032
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q5_0/Q5_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 659d97c49ee3f90d208159f798aaff037e550e09
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q5_0.gguf
      size: 4651691712
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: ddfd81b7f884a041ddcf2b8f7fac4b42d2753581
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q5_K_M.gguf
      size: 4783156928
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 231af6cff6a5798520a7f85f57663e4183f77a2a
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q5_K_S.gguf
      size: 4651691712
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q6_K/Q6_K.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: 1ae892621f15436b4f0b5825bf5a7ced30e657ad
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q6_K.gguf
      size: 5529194176
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/Vigogne-2-7B-Chat/Q8_0/Q8_0.gguf
    - --ctx-size
    - "4096"
    - --n-gpu-layers
    - "35"
    artifacts:
    - id: d5b13d1d11cfec53e38ba22e055b73be74d891e6
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGUF/resolve/main/vigogne-2-7b-chat.Q8_0.gguf
      size: 7161089728
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 4096
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "Below is a conversation between a user and an AI assistant named Vigogne.\nVigogne is polite, emotionally aware, humble-but-knowledgeable, always providing helpful and detailed answers.\nVigogne is skilled in responding proficiently in the languages its users use and can perform a wide range of tasks such as text editing, translation, question answering, logical reasoning, coding, and many others.\nVigogne cannot receive or generate audio or visual content and cannot access the internet.\nVigogne strictly avoids discussing sensitive, offensive, illegal, ethical, or political topics and caveats when unsure of the answer.\n\n<|UTILISATEUR|>: {prompt}\n<|ASSISTANT|>: \n"
