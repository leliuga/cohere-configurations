id: openbuddy-mistral-7B-v13
parent_id: OpenBuddy/openbuddy-mistral-7b-v13
name: Openbuddy Mistral 7B v13
description: ""
architecture: mistral
licence: apache-2.0
object: model
owned_by: OpenBuddy
pipeline: text-generation
languages: []
tags:
- transformers
- gguf
- mistral
- text-generation
- zh
- en
- fr
- de
- ja
- ko
- it
- ru
- base_model:OpenBuddy/openbuddy-mistral-7b-v13
- license:apache-2.0
- has_space
- text-generation-inference
- region:us
config:
  vocab_size: 36608
  context_size: 32768
  embedding_size: 4096
  attention_head_size: 32
  key_value_head_size: 8
  intermediate_size: 14336
  hidden_layer_size: 32
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q2_K.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: f4f06b0b4ca40b21bc67ffc66dc2dc37361d0c8c
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q2_K.gguf
      size: 3104861952
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q3_K_L.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 0652f7db35af850711e6f51fafccaa17b6b23ae7
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q3_K_L.gguf
      size: 3845705472
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q3_K_M.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: b7b151470b35c89b7b797eeb7c995f0dc4590f57
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q3_K_M.gguf
      size: 3542667008
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q3_K_S.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 350ecd6b51e92620540152a1f39bbd39e47f54e9
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q3_K_S.gguf
      size: 3188248320
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q4_0.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: ebec212446d9a4622a8b282328422572a4a1c515
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q4_0.gguf
      size: 4135104256
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q4_K_M.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 7b97c1168a5a24d494a0a35390e54bbe661f4524
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q4_K_M.gguf
      size: 4394626816
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q4_K_S.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: ded53f7da64b3b14ecb8bc9923fe04270eea8e49
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q4_K_S.gguf
      size: 4166561536
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q5_0.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: bdc4c6fadbb32e72359c9016dfe5a9279a5c422a
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q5_0.gguf
      size: 5026262784
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q5_K_M.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: caf9c9c80465d2092c47b4880cbaba96cf73e779
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q5_K_M.gguf
      size: 5159956224
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q5_K_S.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 46d6b8c8985cb8ec74d79babf5400b022e568d28
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q5_K_S.gguf
      size: 5026262784
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q6_K.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: 25662ba17519ba70d00e820d8c6d8e484061c8a9
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q6_K.gguf
      size: 5973118720
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/openbuddy-mistral-7B-v13/Q8_0.gguf
    - --ctx-size
    - "32768"
    - --batch-size
    - "4096"
    - --n-gpu-layers
    - "35"
    - --parallel
    - "4"
    - --no-mmap
    - "false"
    - --mlock
    - "true"
    - --numa
    - "true"
    - --cont-batching
    - "true"
    - --embedding
    - "false"
    artifact:
    - id: fec99ada06bda18693b39e3000f9476b53445998
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q8_0.gguf
      size: 7736053504
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 32768
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. You are talking to a human User.\nAlways answer as helpfully and logically as possible, while being safe. Your answers should not include any harmful, political, religious, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\nYou like to use emojis. You can speak fluently in many languages, for example: English, Chinese.\nYou cannot access the internet, but you have vast knowledge, cutoff: 2021-09.\nYou are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.\n\nUser: {prompt}\nAssistant: \n"
