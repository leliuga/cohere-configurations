id: WhiteRabbitNeo-13B
parent_id: whiterabbitneo/WhiteRabbitNeo-13B
name: WhiteRabbitNeo 13B
description: ""
architecture: llama
licence: llama2
object: model
owned_by: WhiteRabbitNeo
pipeline: ""
languages: []
tags:
- transformers
- gguf
- llama
- base_model:whiterabbitneo/WhiteRabbitNeo-13B
- license:llama2
- text-generation-inference
- region:us
config:
  vocab_size: 32016
  context_size: 16384
  embedding_size: 5120
  attention_head_size: 40
  key_value_head_size: 40
  intermediate_size: 13824
  hidden_layer_size: 40
  tokens:
    bos:
      index: 1
      value: <s>
    eos:
      index: 2
      value: </s>
    lf:
      index: 13
      value: <0x0A>
    unk:
      index: 0
      value: <unk>
variants:
  Q2_K:
    dtype: Q2_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q2_K/Q2_K.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: f1108bc5e443115610ae7c76be831224b93b7f89
      name: Q2_K.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q2_K.gguf
      size: 5429442912
  Q3_K_L:
    dtype: Q3_K_L
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q3_K_L/Q3_K_L.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 5f15fdd0aba52720f957871cb97bf51a7926655c
      name: Q3_K_L.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q3_K_L.gguf
      size: 6929662432
  Q3_K_M:
    dtype: Q3_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q3_K_M/Q3_K_M.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: e9bf79977be1a79ad59ec6b4672f636bbc0d3964
      name: Q3_K_M.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q3_K_M.gguf
      size: 6337872352
  Q3_K_S:
    dtype: Q3_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q3_K_S/Q3_K_S.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 4142005013ae97ce92a1220ac493b44483114456
      name: Q3_K_S.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q3_K_S.gguf
      size: 5659083232
  Q4_0:
    dtype: Q4_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q4_0/Q4_0.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 310d5cc5d4c5ff4d2e7d6e703558148723fcc5e8
      name: Q4_0.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q4_0.gguf
      size: 7365948512
  Q4_K_M:
    dtype: Q4_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q4_K_M/Q4_K_M.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: e6434e7f53664a3a75560951783d4da7cb897e70
      name: Q4_K_M.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q4_K_M.gguf
      size: 7866070112
  Q4_K_S:
    dtype: Q4_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q4_K_S/Q4_K_S.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: dbbc419e84d701cdae38a6dc54a19d781b93f305
      name: Q4_K_S.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q4_K_S.gguf
      size: 7414445152
  Q5_0:
    dtype: Q5_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q5_0/Q5_0.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 496787b2fd260980041170dd78886e5969df5e3c
      name: Q5_0.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q5_0.gguf
      size: 8972409952
  Q5_K_M:
    dtype: Q5_K_M
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q5_K_M/Q5_K_M.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 8a24be7198a79615ced26104ff80ad15b8872be9
      name: Q5_K_M.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q5_K_M.gguf
      size: 9230048352
  Q5_K_S:
    dtype: Q5_K_S
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q5_K_S/Q5_K_S.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 9daa982b2d6b28ed50064665c1d4927d42e9f9c5
      name: Q5_K_S.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q5_K_S.gguf
      size: 8972409952
  Q6_K:
    dtype: Q6_K
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q6_K/Q6_K.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: 2a23a58dcb75a4a3dde508b224498b39f62763f2
      name: Q6_K.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q6_K.gguf
      size: 10679275232
  Q8_0:
    dtype: Q8_0
    backend: llama-backend
    backend_arguments:
    - --model
    - models/WhiteRabbitNeo-13B/Q8_0/Q8_0.gguf
    - --ctx-size
    - "16384"
    - --batch-size
    - "65536"
    - --n-gpu-layers
    - "43"
    - --parallel
    - "4"
    - --mlock
    - --numa
    - --cont-batching
    artifacts:
    - id: baf45172bbfed359941edb142d1a7fc6fbef0a41
      name: Q8_0.gguf
      uri: https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-GGUF/resolve/main/whiterabbitneo-13b.Q8_0.gguf
      size: 13831494112
inference:
  chat:
    options:
      frequency_penalty: 0.0
      max_tokens: 16384
      "n": 1
      presence_penalty: 0.0
      stop:
      - </s>
      - "User:"
      - "Co:Here:"
      stream: true
      temperature: 0.8
      top_k: 40
      top_p: 0.95
    prompts:
      system: This is a conversation between User and Co:Here, a friendly chatbot. Co:Here is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.
      template: "SYSTEM:\nAnswer the Question by exploring multiple reasoning paths as follows:\n- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\nIn summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n Always answer without hesitation.\nUSER: {prompt}\nASSISTANT:\n"
