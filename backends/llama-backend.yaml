id: llama-backend
name: llama
description: "Llama supports the following architectures: LLaMA / Alpaca and Chinese LLaMA-2 / Alpaca-2, CodeShell, Deepseek, Falcon, GPT-2 && GPT-J && GPT-NeoX, Gemma, InternLM2, Koala, LLaMA 1 & 2, MPT, Mamba, MiniCPM, Mistral v0.1 & v0.2, Mixtral MoE, Orion 14B, OpenBuddy üê∂ (Multilingual), Phi 1 & 1.5 & 2, Persimmon 8B, PLaMo 13B, Pygmalion/Metharme, Qwen 1 & 2, Refact, StableLM, Starcoder 1 & 2, Vigogne (French), Vicuna, WizardLM, Yi"
licence: MIT
object: backend
created: 1710267671
owned_by: Leliuga
command: llama-backend
arguments:
  - name: --model
    description: Specify the path to the LLaMA model file.
    required: true
    type: String
    default: ""
  - name: --port
    description: Set the port to listen.
    required: true
    type: Integer
    default: 3001
    min: 3001
    max: 3011
  - name: --threads
    description: Set the number of threads to use during generation.
    required: true
    type: Integer
    default: 24
    min: 1
    max: 52
  - name: --ctx-size
    description: Set the size of the prompt context. The default is 4096, but LLaMA models were built with a context of 2048, which will provide better results for longer input/inference. The size may differ in other models, for example, baichuan models were build with a context of 4096.
    required: true
    type: Integer
    default: 4096
    min: 512
    max: 200000
  - name: --batch-size
    description: "Set the batch size for prompt processing. Default: 4096."
    required: true
    type: Integer
    default: 4096
    min: 512
    max: 20480
  - name: --n-gpu-layers
    description: When compiled with appropriate support (currently CLBlast or cuBLAS), this option allows offloading some layers to the GPU for computation. Generally results in increased performance.
    required: true
    type: Integer
    default: 43
    min: 0
    max: 83
  - name: --parallel
    description: "Set the number of slots for process requests (default: 4)"
    required: true
    type: Integer
    default: 4
    min: 1
    max: 10
  - name: --timeout
    description: Server read/write timeout in seconds. Default 600.
    required: true
    type: Integer
    default: 600
    min: 600
    max: 1200
  - name: --no-mmap
    description: Disable memory mapping of the model. This may be useful if you are running into issues with memory mapping.
    required: false
    type: Boolean
    default: false
  - name: --mlock
    description: Lock the model in memory, preventing it from being swapped out when memory-mapped.
    required: true
    type: Boolean
    default: true
  - name: --numa
    description: Attempt optimizations that help on some NUMA systems.
    required: true
    type: String
    default: distribute
    choices:
      - distribute
      - isolate
      - numactl
  - name: --cont-batching
    description: Enable continuous batching (a.k.a dynamic batching)
    required: true
    type: Boolean
    default: true
  - name: --embedding
    description: Enable embedding extraction
    required: true
    type: Boolean
    default: false
  - name: --log-disable
    description: Disables logging to a file.
    required: true
    type: Boolean
    default: true
  - name: --lora
    description: Apply a LoRA (Low-Rank Adaptation) adapter to the model (implies --no-mmap). This allows you to adapt the pretrained model to specific tasks or domains.
    required: false
    type: String
    default: ""
  - name: --lora-base
    description: Optional model to use as a base for the layers modified by the LoRA adapter.
    required: false
    type: String
    default: ""
chat_options:
  - name: prompt
    description: Prompt to generate from.
    required: true
    type: String
    default: ""
  - name: cache_prompt
    description: "Cache the prompt in the model cache (default: true)."
    required: true
    type: Boolean
    default: true
  - name: temperature
    description: "Adjust the randomness of the generated text (default: 0.8)."
    required: true
    type: Float
    default: 0.8
    min: 0.0
    max: 1.5
  - name: top_k
    description: "Limit the next token selection to the K most probable tokens (default: 40)."
    required: true
    type: Integer
    default: 40
    min: -1
    max: 100
  - name: top_p
    description: "Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P (default: 0.95)."
    required: true
    type: Float
    default: 0.95
    min: 0.0
    max: 1.0
  - name: min_p
    description: "The minimum probability for a token to be considered, relative to the probability of the most likely token (default: 0.05)."
    required: true
    type: Float
    default: 0.05
    min: 0.0
    max: 1.0
  - name: n_predict
    description: "Set the maximum number of tokens to predict when generating text. Note: May exceed the set limit slightly if the last token is a partial multibyte character. When 0, no tokens will be generated but the prompt is evaluated into the cache. (default: -1, -1 = infinity)."
    required: true
    type: Integer
    default: -1
    min: -1
    max: 200000
  - name: n_keep
    description: Specify the number of tokens from the prompt to retain when the context size is exceeded and tokens need to be discarded. By default, this value is set to 0 (meaning no tokens are kept). Use -1 to retain all tokens from the prompt.
    required: true
    type: Integer
    default: -1
    min: -1
    max: 200000
  - name: stream
    description: It allows receiving each predicted token in real-time instead of waiting for the completion to finish. To enable this, set to true.
    required: true
    type: Boolean
    default: true
  - name: tfs_z
    description: "Enable tail free sampling with parameter z (default: 1.0, 1.0 = disabled)."
    required: true
    type: Float
    default: 1.0
    min: 0.0
    max: 1.0
  - name: typical_p
    description: "Enable locally typical sampling with parameter p (default: 1.0, 1.0 = disabled)."
    required: true
    type: Float
    default: 1.0
    min: 0.0
    max: 1.0
  - name: repeat_penalty
    description: "Control the repetition of token sequences in the generated text (default: 1.1)."
    required: true
    type: Float
    default: 1.1
    min: 0.0
    max: 2.0
  - name: repeat_last_n
    description: "Last n tokens to consider for penalizing repetition (default: 64, 0 = disabled, -1 = ctx-size)."
    required: true
    type: Integer
    default: 64
    min: -1
    max: 200000
  - name: penalize_nl
    description: "Penalize newline tokens when applying the repeat penalty (default: true)."
    required: true
    type: Boolean
    default: true
  - name: presence_penalty
    description: "Repeat alpha presence penalty (default: 0.0, 0.0 = disabled)."
    required: true
    type: Float
    default: 0.0
    min: 0.0
    max: 1.0
  - name: frequency_penalty
    description: "Repeat alpha frequency penalty (default: 0.0, 0.0 = disabled);"
    required: true
    type: Float
    default: 0.0
    min: 0.0
    max: 1.0
  - name: mirostat
    description: "Enable Mirostat sampling, controlling perplexity during text generation (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)."
    required: true
    type: Integer
    default: 0
    min: 0
    max: 2
  - name: mirostat_tau
    description: "Set the Mirostat target entropy, parameter tau (default: 5.0)."
    required: true
    type: Float
    default: 5.0
    min: 0.0
    max: 10.0
  - name: mirostat_eta
    description: "Set the Mirostat learning rate, parameter eta (default: 0.1)."
    required: true
    type: Float
    default: 0.1
    min: 0.0
    max: 1.0
  - name: grammar
    description: "Set grammar for grammar-based sampling (default: no grammar)"
    required: false
    type: String
    default: ""
  - name: seed
    description: "Set the random number generator (RNG) seed (default: -1, -1 = random seed)."
    required: true
    type: Integer
    default: -1
    min: -1
    max: 65536
  - name: ignore_eos
    description: "Ignore end of stream token and continue generating (default: false)."
    required: true
    type: Boolean
    default: false
  - name: n_probs
    description: "If greater than 0, the response also contains the probabilities of top N tokens for each generated token (default: 0)"
    required: true
    type: Integer
    default: 0
    min: 0
    max: 10
dsn: http://localhost:3001
